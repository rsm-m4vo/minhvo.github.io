---
title: "Poisson Regression Examples"
author: "Your Name"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

We begin by loading the Blueprinty dataset, which includes 1,500 engineering firms. For each firm, we have the number of patents awarded over the past 5 years, whether or not the firm uses Blueprinty's software, and basic firm characteristics such as region and age.

```{r}
library(readr)
library(dplyr)
library(ggplot2)

# Load the dataset
df <- read_csv("../../data/blueprinty.csv")

# Preview
glimpse(df)
summary(df)
```
```{r}
# Compare average patents by customer status
df %>%
  group_by(iscustomer) %>%
  summarise(mean_patents = mean(patents),
            n = n())

# Histogram of patents by customer status
ggplot(df, aes(x = patents, fill = factor(iscustomer))) +
  geom_histogram(binwidth = 1, position = "dodge", color = "white") +
  labs(x = "Number of Patents", y = "Count", fill = "Customer Status") +
  scale_fill_manual(values = c("darkorange", "steelblue"), 
                    labels = c("Non-Customer", "Customer")) +
  theme_minimal()
```

From the summary statistics and histogram, we observe that firms using Blueprinty's software tend to have a higher average number of patents compared to those who do not. The distribution is right-skewed for both groups, but the customer group appears to have more firms with higher patent counts. While this is suggestive of a possible positive effect, further analysis using a model like Poisson regression will help us better assess the relationship while controlling for other factors.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{r}
# Compare average age by customer status
df %>%
  group_by(iscustomer) %>%
  summarise(mean_age = mean(age),
            median_age = median(age),
            sd_age = sd(age),
            n = n())

# Boxplot of age by customer status
ggplot(df, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +
  geom_boxplot() +
  labs(x = "Customer Status", y = "Firm Age (Years)", fill = "Customer Status") +
  scale_fill_manual(values = c("darkorange", "steelblue"),
                    labels = c("Non-Customer", "Customer")) +
  scale_x_discrete(labels = c("Non-Customer", "Customer")) +
  theme_minimal()

# Compare region distributions
table(df$region, df$iscustomer)

# Region as barplot
ggplot(df, aes(x = region, fill = factor(iscustomer))) +
  geom_bar(position = "fill") +
  labs(x = "Region", y = "Proportion", fill = "Customer Status") +
  scale_fill_manual(values = c("darkorange", "steelblue"),
                    labels = c("Non-Customer", "Customer")) +
  theme_minimal()
```

When comparing firm age, we see that Blueprinty customers tend to be older on average than non-customers. This suggests that more established firms may be more likely to invest in specialized software. In terms of regional distribution, the stacked bar chart shows that Blueprinty customers are not evenly spread across regions â€” some regions have a higher concentration of customers than others. These differences in age and region could be driving part of the observed difference in patent counts, so it will be important to control for them in our regression model.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

### Likelihood Function

For a Poisson random variable \( Y \sim \text{Poisson}(\lambda) \), the probability mass function is:

$$
f(Y \mid \lambda) = \frac{e^{-\lambda} \lambda^Y}{Y!}
$$

Given a dataset of \( n \) independent observations \( Y_1, Y_2, \ldots, Y_n \), the likelihood function is:

$$
L(\lambda) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Taking the logarithm of the likelihood gives the log-likelihood function:

$$
\log L(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log(Y_i!) \right)
$$


```{r}
# Define a Poisson log-likelihood function
poisson_loglikelihood <- function(lambda, Y) {
  if (lambda <= 0) return(-Inf)  # log of 0 or negative not allowed
  sum(-lambda + Y * log(lambda) - lfactorial(Y))
}

# Example: Evaluate log-likelihood at lambda = 2 for some patent counts
sample_Y <- c(0, 1, 3, 2, 0, 5, 1)
poisson_loglikelihood(lambda = 2, Y = sample_Y)
```

```{r}
# Extract Y (number of patents)
Y <- df$patents

# Create a sequence of lambda values to test
lambda_vals <- seq(0.1, 10, length.out = 200)

# Calculate log-likelihoods
loglik_vals <- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y))

# Plot
plot(lambda_vals, loglik_vals, type = "l", col = "steelblue",
     xlab = expression(lambda), ylab = "Log-Likelihood",
     main = "Poisson Log-Likelihood over Lambda")
abline(v = mean(Y), col = "red", lty = 2)  # show where sample mean falls
```

### Analytical Derivation of the MLE

The log-likelihood function for \( n \) independent observations from a Poisson distribution is:

$$
\log L(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log(Y_i!) \right)
$$

Taking the derivative with respect to \( \lambda \):

$$
\frac{d}{d\lambda} \log L(\lambda) = \sum_{i=1}^{n} \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{\sum Y_i}{\lambda}
$$

Setting this equal to 0 and solving for \( \lambda \), we get:

$$
\lambda_{MLE} = \bar{Y}
$$

This confirms our intuition: the MLE for a Poisson mean is simply the sample mean.

```{r}

# Negative log-likelihood for use in minimization
neg_loglik <- function(lambda) {
  -poisson_loglikelihood(lambda, Y)
}

# Optimize using the sample mean as starting point
optim_result <- optim(par = mean(Y), fn = neg_loglik, method = "Brent", lower = 0.01, upper = 20)

# View result
optim_result$par  # MLE
```

The plot of the log-likelihood function shows a clear peak, which occurs near the average number of patents awarded across firms in the dataset. By taking the derivative of the log-likelihood and solving for the value that maximizes it, we find that the maximum likelihood estimate (MLE) for the Poisson rate parameter is simply the sample mean. This makes intuitive sense, since in a Poisson distribution the mean and variance are both equal to the rate parameter. Using numerical optimization confirms this result: the value that maximizes the log-likelihood is nearly identical to the observed average number of patents.

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\lambda_i = e^{X_i'\beta}$. _For example:_

```
poisson_regression_likelihood <- function(beta, Y, X){
   ...
}
```

_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._

_todo: Check your results using R's glm() function or Python sm.GLM() function._

_todo: Interpret the results._ 

_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._




## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._





