---
title: "Poisson Regression Examples"
author: "Your Name"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

We begin by loading the Blueprinty dataset, which includes 1,500 engineering firms. For each firm, we have the number of patents awarded over the past 5 years, whether or not the firm uses Blueprinty's software, and basic firm characteristics such as region and age.

```{r}
library(readr)
library(dplyr)
library(ggplot2)

# Load the dataset
df <- read_csv("../../data/blueprinty.csv")

# Preview
glimpse(df)
summary(df)
```
```{r}
# Compare average patents by customer status
df %>%
  group_by(iscustomer) %>%
  summarise(mean_patents = mean(patents),
            n = n())

# Histogram of patents by customer status
ggplot(df, aes(x = patents, fill = factor(iscustomer))) +
  geom_histogram(binwidth = 1, position = "dodge", color = "white") +
  labs(x = "Number of Patents", y = "Count", fill = "Customer Status") +
  scale_fill_manual(values = c("darkorange", "steelblue"), 
                    labels = c("Non-Customer", "Customer")) +
  theme_minimal()
```

From the summary statistics and histogram, we observe that firms using Blueprinty's software tend to have a higher average number of patents compared to those who do not. The distribution is right-skewed for both groups, but the customer group appears to have more firms with higher patent counts. While this is suggestive of a possible positive effect, further analysis using a model like Poisson regression will help us better assess the relationship while controlling for other factors.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{r}
# Compare average age by customer status
df %>%
  group_by(iscustomer) %>%
  summarise(mean_age = mean(age),
            median_age = median(age),
            sd_age = sd(age),
            n = n())

# Boxplot of age by customer status
ggplot(df, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +
  geom_boxplot() +
  labs(x = "Customer Status", y = "Firm Age (Years)", fill = "Customer Status") +
  scale_fill_manual(values = c("darkorange", "steelblue"),
                    labels = c("Non-Customer", "Customer")) +
  scale_x_discrete(labels = c("Non-Customer", "Customer")) +
  theme_minimal()

# Compare region distributions
table(df$region, df$iscustomer)

# Region as barplot
ggplot(df, aes(x = region, fill = factor(iscustomer))) +
  geom_bar(position = "fill") +
  labs(x = "Region", y = "Proportion", fill = "Customer Status") +
  scale_fill_manual(values = c("darkorange", "steelblue"),
                    labels = c("Non-Customer", "Customer")) +
  theme_minimal()
```

When comparing firm age, we see that Blueprinty customers tend to be older on average than non-customers. This suggests that more established firms may be more likely to invest in specialized software. In terms of regional distribution, the stacked bar chart shows that Blueprinty customers are not evenly spread across regions — some regions have a higher concentration of customers than others. These differences in age and region could be driving part of the observed difference in patent counts, so it will be important to control for them in our regression model.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

### Likelihood Function

For a Poisson random variable \( Y \sim \text{Poisson}(\lambda) \), the probability mass function is:

$$
f(Y \mid \lambda) = \frac{e^{-\lambda} \lambda^Y}{Y!}
$$

Given a dataset of \( n \) independent observations \( Y_1, Y_2, \ldots, Y_n \), the likelihood function is:

$$
L(\lambda) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Taking the logarithm of the likelihood gives the log-likelihood function:

$$
\log L(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log(Y_i!) \right)
$$


```{r}
# Define a Poisson log-likelihood function
poisson_loglikelihood <- function(lambda, Y) {
  if (lambda <= 0) return(-Inf)  # log of 0 or negative not allowed
  sum(-lambda + Y * log(lambda) - lfactorial(Y))
}

# Example: Evaluate log-likelihood at lambda = 2 for some patent counts
sample_Y <- c(0, 1, 3, 2, 0, 5, 1)
poisson_loglikelihood(lambda = 2, Y = sample_Y)
```

```{r}
# Extract Y (number of patents)
Y <- df$patents

# Create a sequence of lambda values to test
lambda_vals <- seq(0.1, 10, length.out = 200)

# Calculate log-likelihoods
loglik_vals <- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y))

# Plot
plot(lambda_vals, loglik_vals, type = "l", col = "steelblue",
     xlab = expression(lambda), ylab = "Log-Likelihood",
     main = "Poisson Log-Likelihood over Lambda")
abline(v = mean(Y), col = "red", lty = 2)  # show where sample mean falls
```

### Analytical Derivation of the MLE

The log-likelihood function for \( n \) independent observations from a Poisson distribution is:

$$
\log L(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log(Y_i!) \right)
$$

Taking the derivative with respect to \( \lambda \):

$$
\frac{d}{d\lambda} \log L(\lambda) = \sum_{i=1}^{n} \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{\sum Y_i}{\lambda}
$$

Setting this equal to 0 and solving for \( \lambda \), we get:

$$
\lambda_{MLE} = \bar{Y}
$$

This confirms our intuition: the MLE for a Poisson mean is simply the sample mean.

```{r}

# Negative log-likelihood for use in minimization
neg_loglik <- function(lambda) {
  -poisson_loglikelihood(lambda, Y)
}

# Optimize using the sample mean as starting point
optim_result <- optim(par = mean(Y), fn = neg_loglik, method = "Brent", lower = 0.01, upper = 20)

# View result
optim_result$par  # MLE
```

The plot of the log-likelihood function shows a clear peak, which occurs near the average number of patents awarded across firms in the dataset. By taking the derivative of the log-likelihood and solving for the value that maximizes it, we find that the maximum likelihood estimate (MLE) for the Poisson rate parameter is simply the sample mean. This makes intuitive sense, since in a Poisson distribution the mean and variance are both equal to the rate parameter. Using numerical optimization confirms this result: the value that maximizes the log-likelihood is nearly identical to the observed average number of patents.

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

```{r}
# Poisson regression log-likelihood
poisson_regression_loglikelihood <- function(beta, Y, X) {
  lambda <- exp(X %*% beta)  # vector of lambda_i values
  loglik <- sum(-lambda + Y * (X %*% beta) - lfactorial(Y))
  return(loglik)
}

```

```{r}
patents ~ age + I(age^2) + region + iscustomer

# Create model matrix (X) and outcome (Y)
X <- model.matrix(~ age + I(age^2) + region + iscustomer, data = df)
Y <- df$patents

# Try initial betas = 0
beta_start <- rep(0, ncol(X))

# Maximize log-likelihood
neg_loglik <- function(beta) -poisson_regression_loglikelihood(beta, Y, X)

# Run optimization
result <- optim(beta_start, neg_loglik, method = "BFGS", control = list(maxit = 1000))

# View estimated coefficients
result$par
```

In this regression-based Poisson model, we allow the expected number of patents awarded to vary across firms based on covariates. The expected value for each firm is modeled as an exponential function of the firm’s characteristics, ensuring that predicted counts remain positive. We include firm age, age squared, regional dummies, and customer status as predictors. The model is estimated by maximizing the log-likelihood function numerically.

```{r}
# Create the model matrix manually
df$agesq <- df$age^2
X <- model.matrix(~ age + agesq + region + iscustomer, data = df)
Y <- df$patents

# Define log-likelihood
poisson_regression_loglikelihood <- function(beta, Y, X) {
  lambda <- exp(X %*% beta)
  sum(-lambda + Y * (X %*% beta) - lfactorial(Y))
}

# Negative log-likelihood for optimization
neg_loglik <- function(beta) -poisson_regression_loglikelihood(beta, Y, X)

# Initial guess
beta_start <- rep(0, ncol(X))

# Maximize using optim
mle_result <- optim(par = beta_start,
                    fn = neg_loglik,
                    method = "BFGS",
                    hessian = TRUE,
                    control = list(maxit = 1000))

# Extract coefficients
beta_hat <- mle_result$par

# Compute standard errors from Hessian
hessian <- mle_result$hessian
vcov <- solve(hessian)  # variance-covariance matrix
se <- sqrt(diag(vcov))

# Combine results into a table
results_table <- data.frame(
  Term = colnames(X),
  Estimate = round(beta_hat, 4),
  StdError = round(se, 4)
)

results_table
```

```{r}
glm_result <- glm(patents ~ age + I(age^2) + region + iscustomer,
                  data = df, family = poisson(link = "log"))

summary(glm_result)
```

The Poisson regression results suggest that firm age and being a Blueprinty customer are significant predictors of patent counts. The customer coefficient is positive and statistically significant, indicating that — controlling for age and region — Blueprinty users tend to receive more patents. Age has a nonlinear effect: the positive coefficient on age and negative coefficient on age squared suggests that patent productivity increases with firm age up to a point, then declines. Regional effects vary, with some regions having significantly different baseline patenting levels. These results support the marketing team’s hypothesis, but causality should be interpreted cautiously due to potential selection bias in who becomes a customer.

```{r}
# Predicted values using fitted beta_hat from optim()
# (assumes you've already run your optimization and stored beta_hat)

# Create counterfactual design matrices
X_0 <- X
X_0[, "iscustomer"] <- 0

X_1 <- X
X_1[, "iscustomer"] <- 1

# Predicted counts under both scenarios
y_pred_0 <- exp(X_0 %*% beta_hat)
y_pred_1 <- exp(X_1 %*% beta_hat)

# Difference in predicted patent counts
pred_diff <- y_pred_1 - y_pred_0

# Average treatment effect of being a Blueprinty customer
avg_effect <- mean(pred_diff)
avg_effect
```

To quantify the effect of Blueprinty’s software on patenting success, we computed predicted patent counts for each firm under two hypothetical scenarios: one where no firm used the software, and one where all firms did. The average difference in predicted patents per firm was approximately r round(avg_effect, 3), indicating that — on average — Blueprinty customers are expected to receive that many more patents over a five-year period than non-customers, all else equal. While this supports the marketing team’s claim, we emphasize that the relationship is correlational, not necessarily causal.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


```{r}
# Load data
airbnb <- read_csv("../../data/airbnb.csv")

# Quick look at structure and missingness
glimpse(airbnb)
summary(airbnb)

# Drop observations with missing values in key variables
airbnb_clean <- airbnb %>%
  select(number_of_reviews, room_type, bathrooms, bedrooms, price,
         review_scores_cleanliness, review_scores_location, review_scores_value,
         instant_bookable, days) %>%
  na.omit()
```

```{r}
# Distribution of reviews (proxy for bookings)
ggplot(airbnb_clean, aes(x = number_of_reviews)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
  xlim(0, 100) +  # cap for visibility
  labs(title = "Distribution of Number of Reviews", x = "Number of Reviews", y = "Count") +
  theme_minimal()

# Average reviews by room type
airbnb_clean %>%
  group_by(room_type) %>%
  summarise(mean_reviews = mean(number_of_reviews),
            median_reviews = median(number_of_reviews),
            n = n())
```

The number of reviews is right-skewed, with most listings receiving fewer than 50 reviews. Entire apartments tend to have more reviews on average compared to private or shared rooms. There are also missing values in several review score columns, so we've dropped rows with missing values to focus on a clean subset for modeling.

### Poisson Regression Model for Number of Reviews

```{r}
# Convert instant_bookable to binary variable
airbnb_clean <- airbnb_clean %>%
  mutate(instant_bookable = ifelse(instant_bookable == "t", 1, 0))

# Fit the Poisson regression model
poisson_model <- glm(number_of_reviews ~ room_type + price + bathrooms + bedrooms +
                       review_scores_cleanliness + review_scores_location +
                       review_scores_value + instant_bookable + days,
                     data = airbnb_clean, family = poisson())

# View results
summary(poisson_model)
```

### Model Interpretation

We fit a Poisson regression model where the number of reviews serves as a proxy for the number of bookings. Several variables are significant predictors of review count:

- **Room Type**: Compared to shared rooms (the reference category), private rooms and entire apartments are associated with significantly more reviews. This likely reflects stronger demand for greater privacy.
  
- **Price**: The coefficient on price is small but positive, suggesting that more expensive listings may receive slightly more reviews, perhaps reflecting higher quality or more established listings.

- **Review Scores**: Higher cleanliness, location, and value scores are all associated with more reviews, which makes sense as better-rated listings attract more bookings.

- **Instant Bookable**: Listings that are instantly bookable receive significantly more reviews on average, likely because they reduce friction for the renter.

- **Days Listed**: The number of days a listing has been on the platform is strongly positively associated with review count — older listings naturally have more time to accumulate reviews.

Overall, the Poisson model provides insight into how listing characteristics relate to popularity (as proxied by review count). The model supports the idea that ease of booking, listing quality, and room type all play meaningful roles in driving booking activity.
